/*package com.kogentix.dl_poc.source_adaptor

import java.util.Properties

//import org.apache.spark.sql.execution.datasources.csv.InferOptions
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.sql.{DataFrame, Row, SQLContext}
import org.slf4j.LoggerFactory

object FileSourceAdapter {

  val logger = LoggerFactory.getLogger(this.getClass)



  def getSchema(df: DataFrame): String = {
    df.schema.json
  }

  // call preview service
  def getRows(df: DataFrame, offset: Long = 0, limit: Long = 100): Array[String] = {
    df.toJSON.take(limit.toInt)
  }


  def createDfFromFile(sqlContext: SQLContext, hdfsLocation: String, optionMap: Map[String, String],
                       formatReader: String = "com.databricks.spark.csv"): DataFrame = {

    logger.info(s"createDfFromFile hdfsLocation $hdfsLocation, optionMap $optionMap, formatReader $formatReader")

    // Todo use the schema generated by buildOption
    val df = sqlContext.read.format(formatReader).
      options(optionMap).
      load(hdfsLocation)

    df
  }

  def buildOption(sqlContext: SQLContext, hdfsLocation: String, baseOption: Map[String, String]):  Map[String, String] = {

    val lines = sqlContext.sparkContext.textFile(hdfsLocation).take(10)

    //val mergedOptions = CsvFileUtil.buildOption(lines, baseOption)

    val extraOpts = if ( !baseOption.get("useHeader").isDefined ) {
      val options =  Map("delimiter" -> mergedOptions.get("delimiter").get)
      val inferOptions = new InferOptions(lines, options)
      val (useHeader, schema) = inferOptions.useHeader(sqlContext.sparkContext)
      Map("header" -> useHeader.toString)
    } else {
      Map[String, String]()
    }

    logger.info("mergedOptions => " + mergedOptions + ", extraOpts => " + extraOpts)
    val opts = mergedOptions ++ extraOpts
    logger.info("opts => " + opts)
    opts
  }

}
*/